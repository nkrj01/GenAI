{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkrj01/GenAI/blob/main/LSTM_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Introduction**\n",
        "\n",
        "In this notebook, I've developed an LSTM model using the text from \"Alice in Wonderland.\" By inputting seed words into this trained model, it can generate new texts. While the model has shown a capacity to produce somewhat meaningful texts, enhancing the quality of the generated text will require more training data."
      ],
      "metadata": {
        "id": "qOaQSBeFcIJ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXHtYz6El6gi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Dense, LSTM, Input, Embedding, TextVectorization, Dropout\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVUEIRTGCO-K"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/Colab Notebooks/LSTM_text_generation/pg11.txt'\n",
        "\n",
        "# Use 'with' to ensure the file is properly handled\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    # Read the content of the file\n",
        "    file_content = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgCUVLYKn4zA"
      },
      "outputs": [],
      "source": [
        "def get_tokens(sentence):\n",
        "  sentence = sentence.encode('utf-8').decode('unicode_escape').encode('ascii', 'ignore').decode()\n",
        "  sentence = sentence.replace('\\n', ' ')\n",
        "  sentence = sentence.replace(\"_\", \"\")\n",
        "  translation_table = str.maketrans('', '', string.punctuation)\n",
        "  sentence = sentence.translate(translation_table)\n",
        "  words = word_tokenize(sentence)\n",
        "  return words\n",
        "\n",
        "tokens = get_tokens(file_content)\n",
        "# print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCb3MraQGZtM"
      },
      "source": [
        "### **Creating training data which containes equal length sequences (2 tokens) from the corpus and the correposnding next token to predict (x and y)**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygcE7uIHzSw5"
      },
      "outputs": [],
      "source": [
        "x = []\n",
        "y = []\n",
        "sequence_length = 2\n",
        "for i in range(sequence_length, len(tokens)):\n",
        "    x.append(tokens[i-sequence_length:i])\n",
        "    y.append(tokens[i])\n",
        "    # if i<= sequence_length+1:\n",
        "    #     print(x)\n",
        "    #     print(y)\n",
        "\n",
        "print(\"total number of sequences: \", len(x))\n",
        "x = [[\" \".join(tokens)] for tokens in x]\n",
        "# print(\"First sequence: \", x[0])\n",
        "# print(\"Second sequence: \", x[1])\n",
        "\n",
        "training_data_len = int(np.ceil( len(tokens) * .99 ))\n",
        "x_train = x[0:training_data_len]\n",
        "x_train = np.array(x_train)\n",
        "y_train_words = y[0:training_data_len]\n",
        "\n",
        "print(\"train data length: \", len(x_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77aWBKNDHWqW"
      },
      "source": [
        "###**Data Preparation and preprocessing**\n",
        "\n",
        "1.   Creating TextVectorization layer for tokenzation and Embeddings\n",
        "2.   Transforming labels from words to tokens\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_qQQE0_Td0B",
        "outputId": "e95f4764-c4c6-4e3b-c3b3-728f1deeafcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary lenght:  2717\n"
          ]
        }
      ],
      "source": [
        "def text_cleaning(text):\n",
        "  text_tf = tf.convert_to_tensor(text, dtype=tf.string)\n",
        "  text_tf = tf.strings.lower(text_tf)\n",
        "  return text_tf\n",
        "\n",
        "max_features = 3000\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=text_cleaning,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "vectorize_layer.adapt(x_train)\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "print(\"Vocabulary lenght: \", len(vocab))\n",
        "# print(vocab)\n",
        "\n",
        "def transform_labels(y):\n",
        "  words_tf = tf.data.Dataset.from_tensor_slices(y)\n",
        "  tokens = words_tf.map(lambda x: vectorize_layer(x))\n",
        "  token_list = []\n",
        "  for token in tokens:\n",
        "    token_list.append(token.numpy()[0])\n",
        "  token_list = np.array(token_list).reshape(-1, 1).astype(np.float32)\n",
        "  return token_list\n",
        "\n",
        "y_train = transform_labels(y_train_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgE3XvBjHxc1"
      },
      "source": [
        "### **Model Architecture, compilation, and Fit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWmN6NbNWBMC"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 128\n",
        "inputs = Input(shape=(1,), dtype=tf.string)\n",
        "x = vectorize_layer(inputs)\n",
        "x = Embedding(max_features+1, embedding_dim)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = LSTM(128, return_sequences=False)(x)\n",
        "output = Dense(len(vocab), activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs, output)\n",
        "\n",
        "# input = tf.convert_to_tensor(x_train[0:2], dtype=tf.string)\n",
        "# tokens = model(input)\n",
        "# print(tokens)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          batch_size=32,\n",
        "          epochs=100,\n",
        "          )\n",
        "\n",
        "# Save the model\n",
        "model.save('/content/drive/MyDrive/Colab Notebooks/LSTM_text_generation/my_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQvjd5EuIJ78"
      },
      "source": [
        "### **Text generation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSam1PbbbO9G"
      },
      "outputs": [],
      "source": [
        "def invert_vectorization(encoded_sequence, vocabulary):\n",
        "    return [vocabulary[i] for i in encoded_sequence]\n",
        "\n",
        "def generate_text (seed_string, len_sequence=20):\n",
        "  output_string = \"\"\n",
        "  for i in range(len_sequence):\n",
        "    if i == 0:\n",
        "      seed_tokens = get_tokens(seed_string)[0:sequence_length]\n",
        "\n",
        "    seed_sequence = [\" \".join(seed_tokens)]\n",
        "    seed_sequence = np.array(seed_sequence).reshape(-1, 1)\n",
        "    seed_sequence_tf = tf.convert_to_tensor(seed_sequence, dtype=tf.string)\n",
        "    model_output = model.predict(seed_sequence_tf)\n",
        "    next_token = np.argmax(model_output, axis=1)\n",
        "    next_word = invert_vectorization(next_token, vocab)\n",
        "    output_string = output_string + \" \" + next_word[0]\n",
        "    seed_tokens.append(next_word[0])\n",
        "    seed_tokens.pop(0)\n",
        "  return output_string\n",
        "\n",
        "seed_string = [\"she pictured\", \"little sister\", \"she would\", \"the simple\", \"king said\", \"little children\"]\n",
        "\n",
        "output_string = []\n",
        "for seed in seed_string:\n",
        "  output_string.append(generate_text(seed, 20))\n",
        "\n",
        "df = pd.DataFrame({\"Seed String\": seed_string, \"Generated string\": output_string})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Results**\n",
        "\n",
        "Below is the table of five different output (20 sequences) generated using five different seed tokens (2 sequences). The model was somewhat able to generate meaningful sentences. However, training on more data set would be necessary to improve the text quality.\n",
        "\n",
        "\n",
        "|index|Seed String|Generated string|\n",
        "|---|---|---|\n",
        "|0|she pictured| such a thing before but she could not think of anything to put it into one of the court and|\n",
        "|1|little sister| but more rather more you promised to tell me said alice a little scream of laughter oh hush the rabbit|\n",
        "|2|she would| have appeared to them and considered a little scream of laughter oh hush the rabbit came near her about the|\n",
        "|3|the simple| rules their friends had taught them such as sure i dont think alice went on in the pool of tears|\n",
        "|4|king said| to herself as she could not think of anything to put it into one of the court and got behind|"
      ],
      "metadata": {
        "id": "U8_hIZFNaH7R"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1njdNI5S5r-Ar45hV1ns-GXJoBsUOePjz",
      "authorship_tag": "ABX9TyOqZId5uM8h3OHfWLTAmYz5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}