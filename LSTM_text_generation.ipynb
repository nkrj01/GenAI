{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkrj01/GenAI/blob/main/LSTM_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Introduction**\n",
        "\n",
        "In this notebook, we trained an LSTM model with an attention mechanism on the text of \"Alice in Wonderland\" with the goal of generating new text in the author's style.\n",
        "\n",
        "We compared two models to assess their effectiveness in replicating the author's style. Both models are identical in architecture, with the only difference being in the text embedding layer.\n",
        "\n",
        "**Model 1**: This model initializes its embedding layer with random values. This means that at the start of training, the word representations are randomly assigned and learn to adjust during the training process.\n",
        "\n",
        "**Model 2**: In contrast, this model begins with embedding vectors generated using OpenAI's Ada embeddings. These pre-trained embeddings provide a more sophisticated starting point for the model. Additionally, these embeddings are fine-tuned during training, allowing the model to adapt the embeddings more closely to the specific style and context of \"Alice in Wonderland\".\n",
        "\n",
        "Below is the table of five different output (20 sequences) generated using five different seed tokens (10 sequences). Overall, Model 2 generated more realistic texts than Model 1\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Table 1:Model 1 | 10 sequences long input | 100 epochs | final loss = ~0.06**\n",
        "\n",
        "|index|Seed String|Generated string|\n",
        "|---|---|---|\n",
        "|0|first she of little alice herself and once again|took up the fan and a little thing she was now about two feet high and was going on shrinking|\n",
        "|1|the tiny hands were upon her knee and the|queen was silent in her pocket and she felt that the first thought that she looked down at her hands|\n",
        "|2|bright eager eyes were looking up into could hear|the white rabbit put on her spectacles and began staring at the hatter who turned pale and fidgeted give your|\n",
        "|3|the very tones of her voice and see that queer|well ive been that there were no use now thought alice theyre poor little thing said alice in a coaxing|\n",
        "|4|little of her head to keep back the wandering|cried the mouse in a deep voice what are said the youth one only say this again is very confusing|\n",
        "\n",
        "\n",
        "\n",
        "**Table 2: Model 2 (ada-embeddings) | 10 sequences long input | 100 epochs | final loss = ~0.02**\n",
        "\n",
        "|index|Seed String|Generated string|\n",
        "|---|---|---|\n",
        "|0|first she of little alice herself and once again|alice thought she might as well wait as she had nothing else to do and perhaps after all it might|\n",
        "|1|the tiny hands were upon her knee and the|jury all brightened up again please your majesty said the knave i didnt write it and they cant prove i|\n",
        "|2|bright eager eyes were looking up into could hear|the rabbit say to itself oh dear oh dear i shall be late when she thought it over afterwards it|\n",
        "|3|the very tones of her voice and see that queer|first to be everything is today in it but there were any more breadandbutter and then said so she stood|\n",
        "|4|little of her head to keep back the wandering|and if it makes me grow larger i can reach the key and if it makes me grow smaller i|\n",
        "\n"
      ],
      "metadata": {
        "id": "qOaQSBeFcIJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install cohere\n",
        "! pip install openai"
      ],
      "metadata": {
        "id": "4hg_uOIpOwyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXHtYz6El6gi"
      },
      "outputs": [],
      "source": [
        "# import ast\n",
        "# import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Dense, LSTM, Input, Embedding, Attention, Flatten, Bidirectional\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import re\n",
        "from rich.console import Console\n",
        "console = Console()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NVUEIRTGCO-K"
      },
      "outputs": [],
      "source": [
        "# import the book\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/LSTM_text_generation/pg11.txt'\n",
        "\n",
        "# Use 'with' to ensure the file is properly handled\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    # Read the content of the file\n",
        "    file_content = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Pre-process and clean up\n",
        "2. Tokenization\n",
        "3. Creating x and y data points for model training"
      ],
      "metadata": {
        "id": "tRoclIYILocL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dgCUVLYKn4zA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "dd227c64-a607-49d6-9743-04c4a1b6d77e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mtotal number of sequences:  \u001b[0m\u001b[1;36m25973\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">total number of sequences:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25973</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mFirst sequence: \u001b[0m\n",
              "\u001b[1m[\u001b[0m\u001b[32m'chapter'\u001b[0m\u001b[1m, \u001b[0m\u001b[32m'i'\u001b[0m\u001b[1m, \u001b[0m\u001b[32m'down'\u001b[0m\u001b[1m, \u001b[0m\u001b[32m'the'\u001b[0m\u001b[1m, \u001b[0m\u001b[32m'rabbithole'\u001b[0m\u001b[1m, \u001b[0m\u001b[32m'alice'\u001b[0m\u001b[1m, \u001b[0m\u001b[32m'was'\u001b[0m\u001b[1m, \u001b[0m\u001b[32m'beginning'\u001b[0m\u001b[1m, \u001b[0m\u001b[32m'to'\u001b[0m\u001b[1m, \u001b[0m\u001b[32m'get'\u001b[0m\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">First sequence: </span>\n",
              "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'chapter'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'i'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'down'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'the'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'rabbithole'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'alice'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'was'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'beginning'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'to'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'get'</span><span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mFirst label: \u001b[0m\n",
              "\u001b[1m[\u001b[0m\u001b[32m'very'\u001b[0m\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">First label: </span>\n",
              "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'very'</span><span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def get_tokens(sentence):\n",
        "  sentence = sentence.encode('utf-8').decode('unicode_escape').encode('ascii', 'ignore').decode()\n",
        "  sentence = sentence.lower()\n",
        "  sentence = sentence.replace('\\n', ' ')\n",
        "  sentence = sentence.replace(\"_\", \"\")\n",
        "  translation_table = str.maketrans('', '', string.punctuation)\n",
        "  sentence = sentence.translate(translation_table)\n",
        "  words = word_tokenize(sentence)\n",
        "  return words\n",
        "\n",
        "tokens = get_tokens(file_content)\n",
        "# print(\"All tokens: \", tokens)\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "SEQUENCE_LENGTH = 10\n",
        "for i in range(SEQUENCE_LENGTH , len(tokens)):\n",
        "    x.append(tokens[i-SEQUENCE_LENGTH :i])\n",
        "    y.append(tokens[i])\n",
        "    # if i<= sequence_length+1:\n",
        "    #     print(x)\n",
        "    #     print(y)\n",
        "\n",
        "console.print(\"total number of sequences: \", len(x), style=\"bold\")\n",
        "\n",
        "console.print(\"First sequence: \", x[0], style=\"bold\")\n",
        "console.print(\"First label: \", [y[0]], style=\"bold\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Creating vocabulary containing all tokens\n",
        "2. Fitting the tokenizer to create index-token pair dictionary\n",
        "3. Defining a word-to-integer-token encoder\n",
        "4. Creating x_train and y_train by encoding word-tokens to integer-tokens"
      ],
      "metadata": {
        "id": "bKqsrSpNMvq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "counter = Counter()\n",
        "counter.update(tokens)\n",
        "all_words = counter.most_common()\n",
        "vocab = [i[0] for i in all_words]\n",
        "print(\"length of vocabulary: \", len(vocab))\n",
        "\n",
        "# create tokenizer\n",
        "def create_tokenizer(text):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    return tokenizer\n",
        "\n",
        "# fit on vocab\n",
        "tokenizer = create_tokenizer(vocab)\n",
        "\n",
        "# print word index pairs\n",
        "word_index = tokenizer.word_index\n",
        "# print(\"word-index pair: \", word_index)\n",
        "\n",
        "# create encoder function\n",
        "def encode_docs(tokenizer, docs):\n",
        "    # integer encode\n",
        "    encoded = tokenizer.texts_to_sequences(docs)\n",
        "    return encoded\n",
        "\n",
        "# convert texts to encoded sequences\n",
        "x_train = encode_docs(tokenizer, x)\n",
        "y_train = encode_docs(tokenizer, y)\n",
        "y_train = np.array(y_train).flatten().astype(np.int32)\n",
        "\n",
        "print(\"word tokens of first predictor sequence: \", x[0])\n",
        "print(\"integer tokens of first predictor sequence: \", x_train[0])\n",
        "print(\"word token of first label: \",y[0])\n",
        "print(\"integer token of first label: \", y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P-Wjn-Jizb0",
        "outputId": "67113e09-82e6-418f-c806-c472ef608974"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of vocabulary:  2698\n",
            "word tokens of first predictor sequence:  ['chapter', 'i', 'down', 'the', 'rabbithole', 'alice', 'was', 'beginning', 'to', 'get']\n",
            "integer tokens of first predictor sequence:  [293, 9, 36, 1, 812, 10, 13, 269, 3, 99]\n",
            "word token of first label:  very\n",
            "integer token of first label:  27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieving ada-emebddings from openai API for all the words in the vocabulary"
      ],
      "metadata": {
        "id": "A2wbNCbGNt_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "client = OpenAI(api_key=userdata.get('openai'))\n",
        "\n",
        "def getAdaEmbedding(train_text: list, model=\"text-embedding-ada-002\") -> list:\n",
        "  total_size = len(train_text)\n",
        "  batch_end = 0\n",
        "  batch_size = 500\n",
        "  n_steps = int(total_size/batch_size) + 1\n",
        "  ada_embedding = []\n",
        "  for i in range(n_steps):\n",
        "    batch_start = batch_end\n",
        "    batch_end = batch_start+batch_size\n",
        "    if batch_end<=total_size:\n",
        "      pass\n",
        "    else:\n",
        "      batch_end = total_size\n",
        "      batch_size = total_size % batch_size\n",
        "    text = train_text[batch_start:batch_end]\n",
        "    output = client.embeddings.create(input = text, model=model)\n",
        "    for j in range(batch_size):\n",
        "      ada_embedding.append(output.data[j].embedding)\n",
        "  return ada_embedding\n",
        "\n",
        "# get embeddings from openai API\n",
        "ada_embedding = getAdaEmbedding(vocab)\n",
        "ada_embedding = np.array(ada_embedding)\n",
        "print(\"output shape: \", ada_embedding.shape)\n",
        "\n",
        "# since this is a one time job, save the embedding vectors as a csv file\n",
        "file_path_ada = '/content/drive/MyDrive/Colab Notebooks/LSTM_text_generation/ada_embeddings.csv'\n",
        "np.savetxt(file_path_ada, ada_embedding, delimiter=',', fmt='%f')"
      ],
      "metadata": {
        "id": "8XYbz2IoOsIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating embedding layer for both Model 1 and Model 2."
      ],
      "metadata": {
        "id": "uxLFx-6MN_Jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.initializers import Constant\n",
        "# import ada-embedding csv file\n",
        "file_path_ada = '/content/drive/MyDrive/Colab Notebooks/LSTM_text_generation/ada_embeddings.csv'\n",
        "ada_embedding = np.loadtxt(file_path_ada, delimiter=',')\n",
        "EMBEDDING_DIM = ada_embedding.shape[1]\n",
        "ada_embedding = np.vstack([np.zeros(EMBEDDING_DIM), ada_embedding]) # adding row of zeroes for words not in vocab\n",
        "\n",
        "num_words = len(vocab) + 1\n",
        "\n",
        "# create emebdding layer with ada-embedding vectors as initial conditions\n",
        "# these emebddings will be fine-tuned during the training\n",
        "\n",
        "def embedding_layer(initial_conditions):\n",
        "  embedding_layer = Embedding(num_words,\n",
        "                              EMBEDDING_DIM,\n",
        "                              embeddings_initializer=Constant(initial_conditions),\n",
        "                              trainable=True)\n",
        "  return embedding_layer\n",
        "\n",
        "embedding_layer_ada = embedding_layer(ada_embedding)\n",
        "emebdding_layer_normal = embedding_layer(np.random.uniform(-1, 1, (num_words, EMBEDDING_DIM)))"
      ],
      "metadata": {
        "id": "mydengRai9aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgE3XvBjHxc1"
      },
      "source": [
        "Model Architecture, compilation, and Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWmN6NbNWBMC"
      },
      "outputs": [],
      "source": [
        "def create_model(embedding_layer):\n",
        "  inputs = Input(shape=(SEQUENCE_LENGTH,))\n",
        "  l = embedding_layer(inputs)\n",
        "  l = LSTM(128, return_sequences=False)(l)\n",
        "  l = Attention()([l, l])\n",
        "  l = Dense(num_words, activation='softmax')(l)\n",
        "\n",
        "  # create the model\n",
        "  model = keras.Model(inputs, l)\n",
        "\n",
        "  # compile the model\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "  return model\n",
        "\n",
        "# model = create_model(emebdding_layer_ada)\n",
        "model = create_model(emebdding_layer_normal)\n",
        "\n",
        "x_train = tf.convert_to_tensor(x_train) #  list to tensor\n",
        "# Train the model\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          batch_size=32,\n",
        "          epochs=100,\n",
        "          )\n",
        "\n",
        "# save the model\n",
        "# model.save('/content/drive/MyDrive/Colab Notebooks/LSTM_text_generation/my_model')\n",
        "\n",
        "\n",
        "# only for testing\n",
        "'''input = tf.convert_to_tensor(x_train[0:2])\n",
        "tokens = model(input)\n",
        "print(tokens)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQvjd5EuIJ78"
      },
      "source": [
        "Text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSam1PbbbO9G"
      },
      "outputs": [],
      "source": [
        "# read the seed content from a text file and tokenize\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/LSTM_text_generation/seed_sequence.txt'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    # Read the content of the file\n",
        "    seed_content = file.read()\n",
        "\n",
        "seed_tokens = get_tokens(seed_content)\n",
        "# print(seed_tokens)\n",
        "\n",
        "# create SEQUENCE_LENGTH tokens list as an input to the model\n",
        "# encode: text-to-integers\n",
        "count = 0\n",
        "input_sequence = []\n",
        "for i in range(5):\n",
        "  docs = seed_tokens[count:count+SEQUENCE_LENGTH]\n",
        "  seq = tokenizer.texts_to_sequences(docs)\n",
        "  # fill zero for unknown words\n",
        "  for inner_list in seq:\n",
        "    # Check if the inner list is empty\n",
        "    if not inner_list:\n",
        "        # Fill the empty list with zero\n",
        "        inner_list.append(0)\n",
        "\n",
        "  seq = [item for sublist in seq for item in sublist]\n",
        "  input_sequence.append(seq)\n",
        "  count = count + SEQUENCE_LENGTH\n",
        "\n",
        "print(input_sequence[0])\n",
        "\n",
        "def generate_text (input_sequence, generate_text_len=20):\n",
        "    output_tokens = []\n",
        "    for i in range(generate_text_len):\n",
        "      seed_sequence_tf = tf.convert_to_tensor(np.array(input_sequence).reshape(1, SEQUENCE_LENGTH))\n",
        "      model_output = model.predict(seed_sequence_tf)\n",
        "      next_token = np.argmax(model_output, axis=1)\n",
        "      input_sequence.append(next_token[0])\n",
        "      input_sequence.pop(0)\n",
        "      output_tokens.append(next_token[0])\n",
        "\n",
        "    # print(output_tokens)\n",
        "    return output_tokens\n",
        "\n",
        "input_output_pair = {}\n",
        "for input in input_sequence:\n",
        "  # print(\"seed tokens: \", seed)\n",
        "  seed_texts = \" \".join(tokenizer.sequences_to_texts([input]))\n",
        "  print(\"seed_texts: \", seed_texts)\n",
        "  generated_tokens = generate_text(input)\n",
        "  # print(\"generated tokens: \", generated_tokens)\n",
        "  generated_texts = \" \".join(tokenizer.sequences_to_texts([generated_tokens]))\n",
        "  print(\"generated_texts: \", generated_texts)\n",
        "  input_output_pair[seed_texts] = generated_texts\n",
        "\n",
        "\n",
        "df = pd.DataFrame({\"Seed String\": list(input_output_pair.keys()),\n",
        "                   \"Generated string\": list(input_output_pair.values())})\n",
        "df.head()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1njdNI5S5r-Ar45hV1ns-GXJoBsUOePjz",
      "authorship_tag": "ABX9TyM5Si9l8cfm+9gEa4IbGgNz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}